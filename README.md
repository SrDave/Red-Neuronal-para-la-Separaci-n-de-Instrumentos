# üéµ RamDomMusicSeparate: Red Neuronal para la Separaci√≥n de Instrumentos

## Descripci√≥n General
**DeepAudioSplit** es un proyecto de investigaci√≥n y desarrollo que aplica t√©cnicas de **aprendizaje profundo (Deep Learning)** para la **separaci√≥n de fuentes musicales**, permitiendo aislar instrumentos como **voz, bajo, percusi√≥n y acompa√±amiento** a partir de una mezcla est√©reo completa.  
El modelo se basa en una arquitectura tipo **Encoder-Decoder (UNet)** con **m√≥dulos de atenci√≥n** y un **bloque de refinamiento inspirado en modelos de difusi√≥n**, ofreciendo resultados de alta calidad perceptual.

---

## Objetivo del Proyecto
Desarrollar una **alternativa abierta, libre de licencias y altamente personalizable** frente a modelos comerciales de separaci√≥n musical como **Demucs**, **Spleeter** o **Wave-U-Net**.  
El proyecto busca favorecer la **reproducibilidad cient√≠fica**, la **escalabilidad** y el **control completo sobre los hiperpar√°metros y el flujo de entrenamiento**.

---

## Caracter√≠sticas Principales

| Caracter√≠stica | Descripci√≥n |
|----------------|--------------|
| **Arquitectura** | UNet h√≠brida con m√≥dulos de atenci√≥n y difusi√≥n |
| **Dominio de trabajo** | Espectrogramas complejos (STFT) |
| **Entradas** | Magnitud logar√≠tmica de la mezcla est√©reo |
| **Salidas** | Espectrograma de la fuente objetivo (voz, bater√≠a, etc.) |
| **Frecuencia de muestreo** | 16 kHz |
| **Duraci√≥n de segmento (CHUNK_SIZE)** | 4 segundos |
| **Dataset principal** | MUSDB18-HQ |
| **Framework** | PyTorch 2.4.0 |
| **Optimizaci√≥n** | Adam ‚Äî LR: 1e-4 |
| **Funciones de p√©rdida** | Combinaci√≥n de L1 + MSE + p√©rdidas espectrales espec√≠ficas |

---

## Arquitectura del Modelo

### Primera Fase: **UNet Pura**
Red convolucional sim√©trica con *skip connections* enfocada en la reconstrucci√≥n de espectrogramas. Ideal para capturar patrones espectrales y espaciales.

### Segunda Fase: **UNet + Difusi√≥n**
Una segunda red UNet act√∫a como **m√≥dulo de refinamiento**, inspirada en modelos de difusi√≥n (DiffWave, SpecDiff), reduciendo artefactos y mejorando la limpieza del audio resultante.

### Tercera Fase: **UNet + BLSTM/Transformers**
Se integran mecanismos temporales (BLSTM) y de atenci√≥n (Transformers con *positional encoding*) para capturar dependencias a largo plazo y mejorar la coherencia temporal.

---

## Hiperpar√°metros y Configuraci√≥n

```python
BATCH_SIZE = 4
SAMPLE_RATE = 16000
N_FFT = 4096
HOP_LENGTH = 1024
WINDOW = 'hann'
CHUNK_SIZE = 4.0
LEARNING_RATE = 1e-4
EPOCHS = 200
 ```

Los filtros convolucionales utilizados son **asim√©tricos (5x1 y 1x5)**, optimizados para extraer caracter√≠sticas **espectrales** y **temporales** de forma independiente.

---

## Dataset y Preprocesamiento

El modelo utiliza el conjunto **MUSDB18-HQ**, compuesto por **150 canciones** (100 para entrenamiento, 50 para prueba).  
Cada canci√≥n incluye:

- üé§ `vocals.wav`  
- ü•Å `drums.wav`  
- üé∏ `bass.wav`  
- üéπ `other.wav`  
- üé∂ `mixture.wav`

El dataset se procesa mediante la clase **`AudioDataset`**, que:

- Extrae segmentos aleatorios de duraci√≥n configurable.  
- Realiza *data augmentation* (pitch shifting, ruido gaussiano).  
- Admite trabajo en dominio temporal o frecuencial.  
- Normaliza los tensores al rango `[-1, 1]`.

---

## Funci√≥n de P√©rdida

La p√©rdida total combina m√∫ltiples objetivos:

- **L1 + MSE** (reconstrucci√≥n base)  
- **P√©rdida de separaci√≥n espectral**  
- **P√©rdida de m√°scara espectral**  
- **P√©rdida de contraste est√©reo**

**Ecuaci√≥n general:**

\[
L_{total} = \alpha \cdot L_{base} + \beta \cdot L_{sep} + \gamma \cdot L_{mask} + \delta \cdot L_{ch\_diff}
\]

**Valores t√≠picos:**  
`Œ± = 0.7`, `Œ≤ = 0.2`, `Œ≥ = 0.1`, `Œ¥ = 0.05`

---

## Entrenamiento

- Entrenamiento **end-to-end en PyTorch**.  
- Monitorizaci√≥n mediante **hooks** para analizar activaciones y gradientes.  
- **Normalizaci√≥n selectiva** (solo en encoders).  
- Funci√≥n de activaci√≥n: **ReLU**.  
- Entrenamiento principal orientado a la **pista de bater√≠a** como caso base.

---

## Herramientas de Optimizaci√≥n

- Registro de **activaciones y gradientes por capa**.  
- An√°lisis de **estabilidad num√©rica**.  
- **Visualizaci√≥n en tiempo real** de activaciones.  
- Detecci√≥n de **overfitting** y generaci√≥n din√°mica de nuevos datasets.

---

## Resultados Cuantitativos (SDR)

Los resultados se evaluaron utilizando la m√©trica **Signal-to-Distortion Ratio (SDR)**, que mide la calidad perceptual de la separaci√≥n de cada fuente.  
A continuaci√≥n se presentan los valores obtenidos para las pistas de **bajo**, **bater√≠a**, **otros** y **voces** sobre un conjunto de prueba de 10 canciones.

| # | Bass (dB) | Drums (dB) | Other (dB) | Vocals (dB) |
|:-:|:-------------:|:-------------:|:-------------:|:--------------:|
| 1 | 2.49 | 1.87 | 4.33 | 3.61 |
| 2 | 2.65 | 2.87 | 1.30 | 3.29 |
| 3 | 2.70 | 1.58 | 4.30 | 4.21 |
| 4 | 3.66 | 2.78 | 1.51 | 1.82 |
| 5 | 2.39 | 1.02 | 1.01 | 2.80 |
| 6 | 3.77 | 1.38 | 3.39 | 2.67 |
| 7 | 4.21 | 3.10 | 1.23 | 3.40 |
| 8 | 4.04 | 2.10 | 2.21 | 3.99 |
| 9 | 3.52 | 1.59 | 2.72 | 3.13 |
| 10 | 3.86 | 2.05 | 3.11 | 5.08 |
| **Media** | **3.32 dB** | **2.03 dB** | **2.51 dB** | **3.40 dB** |

**Interpretaci√≥n:**  
El modelo muestra un rendimiento m√°s consistente en las pistas de **bajo** y **voces**, mientras que los instrumentos de percusi√≥n presentan una mayor complejidad en la separaci√≥n debido a su naturaleza transitoria.  
Estos resultados reflejan una mejora perceptible frente a modelos base tradicionales, manteniendo una separaci√≥n limpia y sin artefactos significativos.


